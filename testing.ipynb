{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as ag\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#use .clone() to copy data and not get affected by backprop\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(100):\n",
    "    # Forward pass: compute predicted y using operations on Variables; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Variables.\n",
    "    # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n",
    "    # (1,); loss.data[0] is a scalar value holding the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Variables holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent; w1.data and w2.data are Tensors,\n",
    "    # w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are\n",
    "    # Tensors.\n",
    "    r = w1.data.clone()\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "print r\n",
    "print w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ag.Variable(torch.Tensor([7]), requires_grad=True)\n",
    "z_pred = x*4\n",
    "z = ag.Variable(torch.Tensor([29]))\n",
    "loss = nn.MSELoss()\n",
    "l = loss(z_pred,z)\n",
    "l.backward()\n",
    "print x.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    y = x**2\n",
    "    print y.requires_grad\n",
    "    return y\n",
    "\n",
    "x = ag.Variable(torch.Tensor([7]), requires_grad=True)\n",
    "#c = ag.Variable(torch.Tensor([6]), requires_grad=True)\n",
    "z = ag.Variable(torch.Tensor([100]))\n",
    "\n",
    "v1 = f1(x)\n",
    "\n",
    "print v1\n",
    "y=x*2\n",
    "z_pred = f1(y)*4\n",
    "#v2 = f1(x)\n",
    "#print v1,v2\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "l = loss(z_pred,z)\n",
    "l.backward()\n",
    "print x.grad.data\n",
    "print z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "#Making a subset of friends dataset to experiment the pipeline\n",
    "fr = open('Friends-dialogues.txt','r')\n",
    "fw = open('testd.txt','w')\n",
    "c = 0\n",
    "for l in fr:\n",
    "    c+=1\n",
    "    if c>1000:\n",
    "        break\n",
    "    fw.write(l)\n",
    "print c\n",
    "fw.close()\n",
    "fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making counter to pass to vocab file\n",
    "from collections import Counter\n",
    "\n",
    "f = open('testd.txt','r')\n",
    "vc = Counter()\n",
    "for l in f:\n",
    "    vc.update(Counter(l.split()))\n",
    "#print vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors from /home/rohith/Documents/NLP/Dialogue/glove.840B.300d.pt\n",
      "0:00:02.695216\n"
     ]
    }
   ],
   "source": [
    "#creating the vocab object\n",
    "from datetime import datetime\n",
    "import vocab\n",
    "st = datetime.now()\n",
    "vcb = vocab.Vocab(vc, wv_type = \"glove.840B\",min_freq=4)\n",
    "print datetime.now()-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    }
   ],
   "source": [
    "print len(vcb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6694"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(vc.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " 0.8716\n",
      " 0.2908\n",
      " 0.0019\n",
      " 1.5223\n",
      "-0.1070\n",
      " 0.8144\n",
      " 0.4027\n",
      "-2.5946\n",
      " 0.1252\n",
      "-0.8074\n",
      " 1.4320\n",
      "-0.0174\n",
      "-0.3091\n",
      "-0.4098\n",
      " 1.1060\n",
      "-0.4929\n",
      " 0.6959\n",
      "-1.1187\n",
      "-1.5049\n",
      " 0.9039\n",
      "-0.0105\n",
      " 0.3491\n",
      "-1.8101\n",
      " 0.7075\n",
      "-1.3758\n",
      " 0.8249\n",
      " 0.4382\n",
      " 0.4464\n",
      " 0.5642\n",
      " 1.5773\n",
      "-1.7994\n",
      "-0.6132\n",
      " 1.3629\n",
      "-1.9990\n",
      " 0.5763\n",
      "-0.2677\n",
      "-1.2049\n",
      " 1.3254\n",
      "-0.3328\n",
      " 0.5087\n",
      "-1.0881\n",
      "-0.0032\n",
      " 0.5018\n",
      " 0.8686\n",
      " 0.3798\n",
      "-1.1093\n",
      "-1.4197\n",
      "-0.4378\n",
      " 0.2340\n",
      "-1.9110\n",
      "-0.9681\n",
      "-0.8744\n",
      " 0.7364\n",
      " 0.7422\n",
      " 0.0935\n",
      "-1.2482\n",
      "-0.3731\n",
      " 0.8598\n",
      "-0.5227\n",
      " 0.4105\n",
      "-0.9191\n",
      "-0.0411\n",
      " 0.4965\n",
      "-1.2949\n",
      "-1.1591\n",
      "-0.8937\n",
      " 1.0341\n",
      "-0.7163\n",
      "-0.1538\n",
      " 1.0399\n",
      "-0.5660\n",
      "-0.8800\n",
      "-1.2356\n",
      "-1.7444\n",
      " 0.1662\n",
      "-1.4279\n",
      " 1.2391\n",
      "-0.3425\n",
      "-1.2758\n",
      "-1.1501\n",
      " 0.3714\n",
      "-0.1100\n",
      "-0.7125\n",
      "-0.0806\n",
      "-1.4208\n",
      "-0.6153\n",
      "-0.4746\n",
      " 0.7826\n",
      "-0.1784\n",
      "-0.7601\n",
      " 2.4096\n",
      "-1.0755\n",
      "-1.2065\n",
      "-1.1670\n",
      " 0.7543\n",
      "-1.0266\n",
      " 0.1047\n",
      "-1.9392\n",
      "-1.6212\n",
      "-0.9009\n",
      "-0.2818\n",
      " 1.8831\n",
      " 0.2788\n",
      " 0.7098\n",
      " 1.5300\n",
      " 0.1875\n",
      " 2.2754\n",
      "-2.4403\n",
      "-2.2746\n",
      "-0.1048\n",
      " 0.9708\n",
      " 0.7532\n",
      "-1.0815\n",
      " 1.4522\n",
      "-0.6690\n",
      " 0.3554\n",
      "-0.8030\n",
      "-0.2798\n",
      " 1.7356\n",
      "-0.9828\n",
      "-0.2455\n",
      "-1.5503\n",
      "-3.3189\n",
      " 0.3010\n",
      " 1.4799\n",
      " 0.4885\n",
      "-0.5706\n",
      "-1.3422\n",
      "-1.1723\n",
      "-1.7088\n",
      " 0.3116\n",
      "-0.9172\n",
      " 0.1424\n",
      "-0.0981\n",
      "-0.0512\n",
      "-0.1550\n",
      " 0.5814\n",
      "-1.1393\n",
      "-2.3282\n",
      " 0.1045\n",
      "-1.1421\n",
      " 1.0833\n",
      " 1.7956\n",
      "-0.2583\n",
      "-0.7103\n",
      " 1.3206\n",
      "-0.4497\n",
      " 2.0589\n",
      "-0.6667\n",
      "-0.4640\n",
      " 0.3074\n",
      "-0.1899\n",
      " 1.2995\n",
      "-2.2678\n",
      "-0.0267\n",
      " 0.1228\n",
      "-1.1884\n",
      " 0.0416\n",
      "-1.1841\n",
      " 0.0393\n",
      " 0.1550\n",
      " 1.0995\n",
      "-0.5481\n",
      "-0.0508\n",
      "-1.3734\n",
      " 0.5739\n",
      " 0.3446\n",
      "-1.1481\n",
      " 0.3530\n",
      " 1.1244\n",
      "-0.6619\n",
      "-0.4845\n",
      " 0.1922\n",
      " 0.2573\n",
      " 0.4186\n",
      " 0.4501\n",
      "-2.0190\n",
      "-1.8457\n",
      "-0.1370\n",
      "-2.9346\n",
      " 0.1771\n",
      "-0.5684\n",
      "-1.3764\n",
      " 0.4815\n",
      " 0.3232\n",
      " 1.2422\n",
      "-1.1569\n",
      " 0.5503\n",
      "-0.0926\n",
      " 0.3702\n",
      "-1.7200\n",
      "-0.4973\n",
      "-0.5880\n",
      "-0.1524\n",
      "-0.0737\n",
      " 0.7460\n",
      "-0.8423\n",
      "-0.4644\n",
      "-0.2942\n",
      "-1.3666\n",
      "-0.9384\n",
      "-1.6651\n",
      " 1.3384\n",
      " 0.6317\n",
      "-0.5612\n",
      " 0.7618\n",
      " 1.7271\n",
      "-0.6623\n",
      " 3.3380\n",
      " 0.3737\n",
      "-0.3126\n",
      "-0.2413\n",
      "-0.7499\n",
      " 0.6263\n",
      "-0.4245\n",
      "-0.0285\n",
      " 0.7770\n",
      " 1.2571\n",
      " 0.1956\n",
      "-0.5104\n",
      " 0.6464\n",
      "-0.4594\n",
      "-0.7594\n",
      "-0.6891\n",
      "-0.4518\n",
      "-1.3642\n",
      "-0.4173\n",
      " 1.4466\n",
      "-0.8588\n",
      " 0.0192\n",
      " 0.5359\n",
      "-1.1726\n",
      " 0.9063\n",
      " 2.3075\n",
      " 0.2651\n",
      " 0.1937\n",
      "-0.1364\n",
      "-2.3609\n",
      "-2.3877\n",
      "-0.6205\n",
      " 1.2688\n",
      "-0.2820\n",
      "-1.1068\n",
      "-0.8681\n",
      " 0.3251\n",
      " 0.3362\n",
      " 2.0997\n",
      " 1.0382\n",
      "-0.2678\n",
      " 0.0299\n",
      " 0.0530\n",
      "-0.5150\n",
      " 0.5793\n",
      " 0.4242\n",
      " 0.0244\n",
      " 0.2712\n",
      "-0.1965\n",
      " 2.1136\n",
      "-0.4996\n",
      " 0.6474\n",
      "-0.7315\n",
      "-0.6286\n",
      "-0.8333\n",
      "-1.5403\n",
      "-0.6946\n",
      " 0.4535\n",
      " 0.4879\n",
      "-0.4071\n",
      " 2.4335\n",
      "-0.3344\n",
      "-0.6760\n",
      "-0.0430\n",
      "-0.4592\n",
      "-2.2061\n",
      " 0.7980\n",
      " 1.1484\n",
      "-0.9211\n",
      " 1.6431\n",
      " 0.3921\n",
      "-0.5299\n",
      "-1.1186\n",
      "-2.1807\n",
      " 1.1981\n",
      "-1.2367\n",
      "-1.1274\n",
      "-1.3393\n",
      " 2.1076\n",
      "-0.1621\n",
      " 0.3504\n",
      " 0.3649\n",
      "-1.4017\n",
      "-0.2467\n",
      " 1.1228\n",
      "-0.5939\n",
      "-0.0441\n",
      "-0.5815\n",
      "-0.8219\n",
      "-1.7039\n",
      " 1.3337\n",
      " 0.5389\n",
      "[torch.FloatTensor of size 300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = vcb.stoi['asdewrfg']\n",
    "t2 = vcb.vectors[t1]\n",
    "print t1,t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making a subset of friends dataset to experiment the pipeline\n",
    "fr = open('Friends-dialogues.txt','r')\n",
    "fw = open('testd.txt','w')\n",
    "c = 0\n",
    "for l in fr:\n",
    "    c+=1\n",
    "    if c>1000:\n",
    "        break\n",
    "    fw.write(l)\n",
    "print c\n",
    "fw.close()\n",
    "fr.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
