{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as ag\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#use .clone() to copy data and not get affected by backprop\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs, and wrap them in Variables.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Variables during the backward pass.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "# Create random Tensors for weights, and wrap them in Variables.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(100):\n",
    "    # Forward pass: compute predicted y using operations on Variables; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Variables.\n",
    "    # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n",
    "    # (1,); loss.data[0] is a scalar value holding the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Variables holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent; w1.data and w2.data are Tensors,\n",
    "    # w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are\n",
    "    # Tensors.\n",
    "    r = w1.data.clone()\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    # Manually zero the gradients after updating weights\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "print r\n",
    "print w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = ag.Variable(torch.Tensor([7]), requires_grad=True)\n",
    "z_pred = x*4\n",
    "z = ag.Variable(torch.Tensor([29]))\n",
    "loss = nn.MSELoss()\n",
    "l = loss(z_pred,z)\n",
    "l.backward()\n",
    "print x.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    y = x**2\n",
    "    print y.requires_grad\n",
    "    return y\n",
    "\n",
    "x = ag.Variable(torch.Tensor([7]), requires_grad=True)\n",
    "#c = ag.Variable(torch.Tensor([6]), requires_grad=True)\n",
    "z = ag.Variable(torch.Tensor([100]))\n",
    "\n",
    "v1 = f1(x)\n",
    "\n",
    "print v1\n",
    "y=x*2\n",
    "z_pred = f1(y)*4\n",
    "#v2 = f1(x)\n",
    "#print v1,v2\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "l = loss(z_pred,z)\n",
    "l.backward()\n",
    "print x.grad.data\n",
    "print z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a subset of friends dataset to experiment the pipeline\n",
    "fr = open('Friends-dialogues.txt','r')\n",
    "fw = open('testd.txt','w')\n",
    "c = 0\n",
    "for l in fr:\n",
    "    c+=1\n",
    "    if c>1000:\n",
    "        break\n",
    "    fw.write(l)\n",
    "print c\n",
    "fw.close()\n",
    "fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making counter to pass to vocab file\n",
    "from collections import Counter\n",
    "\n",
    "f = open('Friends-dialogues.txt','r')\n",
    "vc = Counter()\n",
    "for l in f:\n",
    "    vc.update(Counter(l.split()))\n",
    "#print vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors from /home/rohith/Documents/NLP/Dialogue/glove.840B.300d.pt\n",
      "0:00:02.161140\n"
     ]
    }
   ],
   "source": [
    "#creating the vocab object\n",
    "from datetime import datetime\n",
    "import vocab\n",
    "st = datetime.now()\n",
    "vcb = vocab.Vocab(vc, wv_type = \"glove.840B\",min_freq=4,specials = [\"EOS\",\"SOS\"])\n",
    "print datetime.now()-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(vcb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(vc.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = vcb.stoi['rod']\n",
    "t2 = vcb.vectors[t1]\n",
    "print t1,t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors from /home/rohith/Documents/NLP/Dialogue/glove.840B.300d.pt\n"
     ]
    }
   ],
   "source": [
    "reload(framework)\n",
    "import framework as fw\n",
    "eg = fw.EmbedGlove('Friends-dialogues.txt',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.1873\n",
      " 0.4060\n",
      "-0.5117\n",
      "-0.5548\n",
      " 0.0397\n",
      " 0.1289\n",
      " 0.4514\n",
      "-0.5915\n",
      " 0.1559\n",
      " 1.5137\n",
      "-0.8702\n",
      " 0.0507\n",
      " 0.1521\n",
      "-0.1918\n",
      " 0.1118\n",
      " 0.1213\n",
      "-0.2721\n",
      " 1.6203\n",
      "-0.2488\n",
      " 0.1406\n",
      " 0.3310\n",
      "-0.0181\n",
      " 0.1524\n",
      "-0.2694\n",
      "-0.2783\n",
      "-0.0521\n",
      "-0.4815\n",
      "-0.5184\n",
      " 0.0863\n",
      " 0.0308\n",
      "-0.2125\n",
      "-0.1138\n",
      "-0.2238\n",
      " 0.1826\n",
      "-0.3454\n",
      " 0.0826\n",
      " 0.1002\n",
      "-0.0795\n",
      "-0.8172\n",
      " 0.0066\n",
      " 0.0801\n",
      "-0.3998\n",
      "-0.0631\n",
      " 0.3226\n",
      "-0.0316\n",
      " 0.4306\n",
      "-0.2727\n",
      "-0.0760\n",
      " 0.1029\n",
      "-0.0887\n",
      "-0.2909\n",
      "-0.0472\n",
      " 0.0460\n",
      "-0.0178\n",
      " 0.0650\n",
      " 0.0885\n",
      "-0.3157\n",
      "-0.5852\n",
      " 0.2229\n",
      "-0.0528\n",
      "-0.5598\n",
      "-0.3958\n",
      "-0.0798\n",
      "-0.0109\n",
      "-0.0417\n",
      "-0.5558\n",
      " 0.0887\n",
      " 0.1371\n",
      "-0.0030\n",
      "-0.0263\n",
      " 0.0773\n",
      " 0.3920\n",
      " 0.3451\n",
      "-0.0801\n",
      " 0.3345\n",
      " 0.2706\n",
      "-0.0245\n",
      " 0.0726\n",
      "-0.1812\n",
      " 0.2369\n",
      " 0.3998\n",
      " 0.4501\n",
      " 0.0272\n",
      " 0.2740\n",
      " 0.1479\n",
      "-0.0058\n",
      " 0.9591\n",
      "-1.0129\n",
      " 0.2070\n",
      " 0.1824\n",
      "-0.2523\n",
      "-0.2626\n",
      "-0.3480\n",
      "-0.0241\n",
      " 0.4447\n",
      " 0.0592\n",
      " 0.4556\n",
      " 0.1970\n",
      "-0.4833\n",
      " 0.0895\n",
      "-0.2237\n",
      "-0.1565\n",
      " 0.2158\n",
      " 0.1167\n",
      " 0.0820\n",
      "-0.8073\n",
      " 0.2390\n",
      "-0.5130\n",
      "-0.3389\n",
      "-0.3150\n",
      "-0.1727\n",
      "-0.6702\n",
      " 0.2710\n",
      "-0.4324\n",
      " 0.0431\n",
      " 0.0212\n",
      " 0.0133\n",
      "-0.0639\n",
      "-0.2496\n",
      "-0.2494\n",
      " 0.3481\n",
      "-0.0713\n",
      " 0.2338\n",
      "-0.0954\n",
      " 0.5249\n",
      " 0.6817\n",
      "-0.1021\n",
      "-0.1491\n",
      "-0.0757\n",
      " 0.1725\n",
      " 0.2544\n",
      " 0.1576\n",
      "-0.5913\n",
      " 0.2430\n",
      " 0.6396\n",
      "-0.0933\n",
      "-0.2791\n",
      "-0.0663\n",
      "-0.0672\n",
      "-0.4093\n",
      "-3.0300\n",
      " 0.1825\n",
      " 0.2011\n",
      " 0.0606\n",
      "-0.2477\n",
      " 0.0553\n",
      "-0.4911\n",
      " 0.3154\n",
      "-0.3423\n",
      "-0.6377\n",
      "-0.3613\n",
      "-0.0590\n",
      " 0.1551\n",
      " 0.0446\n",
      " 0.2357\n",
      "-0.1709\n",
      "-0.2275\n",
      "-0.0232\n",
      " 0.2387\n",
      " 0.0282\n",
      " 0.4297\n",
      "-0.1246\n",
      "-0.0370\n",
      " 0.2006\n",
      "-0.3140\n",
      "-0.0853\n",
      "-0.3350\n",
      "-0.0970\n",
      "-0.1439\n",
      " 0.1115\n",
      "-0.4523\n",
      "-0.2422\n",
      "-0.1824\n",
      "-0.6729\n",
      " 0.0219\n",
      "-0.0548\n",
      "-0.4651\n",
      " 0.4777\n",
      "-0.2475\n",
      "-0.1579\n",
      " 0.1182\n",
      " 0.0569\n",
      "-0.4915\n",
      " 0.1550\n",
      " 0.0164\n",
      " 0.0417\n",
      "-0.3499\n",
      "-0.1598\n",
      " 0.3970\n",
      " 0.2296\n",
      " 0.2469\n",
      " 0.0196\n",
      "-0.2880\n",
      "-0.6998\n",
      " 0.3274\n",
      " 0.1083\n",
      " 0.2494\n",
      "-0.7865\n",
      "-0.0614\n",
      "-0.3736\n",
      "-0.1160\n",
      "-0.2495\n",
      " 0.1016\n",
      " 0.0340\n",
      " 0.1565\n",
      " 0.2134\n",
      "-0.1109\n",
      "-0.0058\n",
      " 0.1787\n",
      "-0.1013\n",
      "-0.0169\n",
      " 0.3000\n",
      "-0.3412\n",
      "-0.0324\n",
      " 0.0425\n",
      " 0.1185\n",
      "-0.1834\n",
      "-0.6287\n",
      "-0.2802\n",
      " 0.4235\n",
      " 0.1128\n",
      " 0.0012\n",
      " 0.1571\n",
      "-0.3632\n",
      "-0.4925\n",
      " 0.1165\n",
      " 0.2402\n",
      " 0.1771\n",
      " 0.0687\n",
      "-0.4414\n",
      "-0.2988\n",
      "-0.0121\n",
      " 0.2833\n",
      " 0.1067\n",
      "-0.1886\n",
      "-0.4135\n",
      "-0.3409\n",
      " 0.0472\n",
      "-0.3831\n",
      " 0.4357\n",
      " 0.2450\n",
      " 0.2734\n",
      "-0.0730\n",
      " 0.4251\n",
      "-0.0325\n",
      "-0.3521\n",
      " 0.4569\n",
      " 0.1943\n",
      "-0.1523\n",
      " 0.4268\n",
      " 0.2880\n",
      "-0.5597\n",
      "-0.1303\n",
      " 0.0898\n",
      " 0.4261\n",
      "-0.1963\n",
      "-0.0720\n",
      "-0.0802\n",
      "-0.3043\n",
      "-0.4619\n",
      " 0.2818\n",
      "-0.0999\n",
      " 0.3510\n",
      " 0.1612\n",
      "-0.0365\n",
      "-0.3674\n",
      "-0.0198\n",
      " 0.3213\n",
      " 0.1748\n",
      " 0.2517\n",
      "-0.0076\n",
      "-0.0938\n",
      "-0.3785\n",
      " 0.4372\n",
      " 0.2129\n",
      " 0.2510\n",
      "-0.1961\n",
      "-0.2887\n",
      "-0.0057\n",
      " 0.4279\n",
      " 0.2062\n",
      "-0.0377\n",
      "-0.1220\n",
      "-0.0793\n",
      "-0.1029\n",
      " 0.0106\n",
      " 0.4988\n",
      " 0.2538\n",
      " 0.1553\n",
      " 0.0018\n",
      " 0.1163\n",
      " 0.0793\n",
      "-0.3914\n",
      "-0.3248\n",
      " 0.6345\n",
      "-0.1891\n",
      " 0.0540\n",
      " 0.1649\n",
      " 0.1876\n",
      " 0.5387\n",
      "[torch.FloatTensor of size 300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print eg(\"I am god\")[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
